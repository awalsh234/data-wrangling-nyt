---
title: "data-wrangling"
author: "Annie Walsh"
date: "2025-07-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Package Setup 

```{r}
wrangling_packages <- c("data.table", "effects", "lme4", "lubridate", "modelsummary", 
                        "psych", "qdap", "reshape", "rJava", "rlang", "skimr", 
                        "syuzhet", "tidytext", "tidyverse", "utils", "yaml")

##brief explanation of packages##
#tidyverse is for wrangling and contains ggplot2, tibble, purrr, dplyr, tidyr, stringr, readr, and forcats
#data.table is for reading large data objects
#skimr and psych are for basic statistics
#rlang and rershape are for basic wrangling
#utils, modelsummary, and yaml are for R notebook functionality
#lubridate is for updating date/time stamps
#qdap, rJava, syuzhet, and tidytext are for text analysis and cleaning
#lme4 is for linear regression exercise

packages <- rownames(installed.packages())
p_to_install <- wrangling_packages[!(wrangling_packages %in% packages)]

if(length(p_to_install) > 0){
  install.packages(p_to_install)
}

lapply(wrangling_packages, library, character.only = TRUE)

```
# Clean environment

```{r}
rm(p_to_install, packages, wrangling_packages)

```

# Check WD

```{r}
#check current working directory
getwd()

wd <- "/Users/annewalsh/Desktop/git_projects/wrangling"
setwd(wd)

rm(wd)
```

# About the Data
Using the publicly-available New York Times Cooking comment data. The data was collected via New York Times’s API.

## Import the data
```{r}
nyt_raw <- data.table::fread("df.csv", 
                 na.strings = c('', 'NA'))
```


```{r}
head(nyt_raw)
```

## Data Organization 

Italicized column names are those we will be examining today:

V1: the index of the comment within a specific recipe
commentID: the comment’s unique identifying number
status: whether or not NYT has approved the comment
commentSequence: the comment’s sequence
userID: the ID number of the user who wrote the comment
userDisplayName: first name or handle selected by user who wrote the comment
userLocation: location of the user (locked)
userTitle: the user’s title, if they have one (e.g., “food critic”; locked)
userURL: the user’s NYT cooking profile URL (locked)
picURL: the URL of the photo included in the comment (locked)
commentTitle: the “subject line”/title of the user’s comment
commentBody: the comment text
createDate: unix time stamp of comment creation date
updateDate: unix time stamp of any comment updates
approveDate: unix time stamp of comment approval date
recommendations: number of other users who liked a specific comment
replyCount: number of replies to a particular comment
replies: replies written to each specific comment
editorsSelection: whether or not the comment has been selected as an “Editor’s Selection” comment
parentID: if reply, comment ID for the original comment (locked)
parentUserDisplay: if reply, user ID for original comment (locked)
depth: comment depth
commentType: original comment vs. reporter reply
trusted: whether or not the comment is trusted
recommendedFlag: whether or not the comment has a recommended flag
permID: permanent comment ID -
isAnonymous: whether or not the comment is anonymous
recipe_id: ID number for the recipe commented on
recipe_name: name of the recipe commented on

## Examine raw data structure

```{r}
#copying the data to a new object so that we can retain the raw data separately
nyt <- nyt_raw
```

```{r}
str(nyt) #shows us what kinds of variables we have

```

Next, we’ll use the names() function to examine the column names of our dataframe, which essentially function as the names of our variables.

```{r}
names(nyt) #shows column names

```
We can look at how frequently each unique value appeared within any given column using the tables() function and specifying the column as the object of the function. Let’s look at how many replies each entry got.

```{r}
table(nyt$replyCount)

```
We can also use it with qualitative variables like recipe names.

```{r}

table(nyt$recipe_name)
```


. . . and we can clean this table up a little bit by converting it into a dataframe

```{r}
data.frame(table(nyt$recipe_name))

```

Next, we can get descriptive statistics for individual numeric variables.

```{r}
mean(nyt$replyCount)

```

```{r}
sd(nyt$replyCount)
```
We can also use visualizations to understand our variables of interest.

```{r}

hist(nyt$replyCount)
```
```{r}
plot(nyt$replyCount)

```
Checking our data for quality assurance could clearly become a lengthy process depending on the number of variables you’re looking at! Here are a few commands to make data exploration more concise:

NOTE: The datasummary_skim function output may appear empty in Markdown, but text is just white and can be seen if highlighted.
```{r}
#skim the entire dataframe using the skimr package
skim(nyt)
```

```{r}
datasummary_skim(nyt,) # create table for R Markdown
```

## Troubleshooting

To search a function 

```{r}
#? [FUNCTION] # use this to learn more about a specific function - see help tab
?hist()
```

## Check your work

Use head() fun to check work at each step in the process

```{r}
head(nyt, n = 10) # specify number of rows desired
```

## Renaming columns 

Raw data here is very messy - give columns more consistent and descriptive names using the rename() function from the dplyr package.

```{r}
names(nyt)
```
```{r}
# Rename individual columns using consistent naming conventions with rest of df
nyt <- dplyr::rename(nyt, 
                     c('recipeName' = 'recipe_name',
                       'recipeID' = 'recipe_id'))

# Or rename columns in a batch!
nyt <- rename_with(nyt, ~ (gsub("user", 
                                "author", 
                                .x, 
                                fixed = TRUE)))

#check work
head(nyt, n = 10)
```

## Selecting Columns and Filtering Rows 

It looks like there are a lot of extra columns we aren’t interested in in this data. We can access the column names directly using the names function (above), and then use dplyr’s select() function to choose the columns we would like to keep. I usually specify that I want to use dplyr’s select() function instead of any select function from another package (for example, there’s also one in the psych package that sometimes conflicts with dplyr) by writing the package name and 2 colons before the function (dplyr::select()).

We can also clean data up to use only rows that we are interested in using filter() or subset() functions. I’m first going to select all rows with “approved” comments by using the filter() function. It’s worth noting that one can filter for rows containing values in a column that are equal to (==) or unequal to (!=) a specific reference value. You can do the same for numeric variables (as we’ll see later on in the “arrange and sort” section), or could also select reference values that are greater than (>)/greater than or equal to (>=), or less than (<)/less than or equal to (<=) a specific reference value.

```{r}
#select by row: this will allow us to keep only comments that have been "approved"
nyt <- filter(nyt, status == "approved") #find status = approved
nrow(nyt) #looks like we didn't lose any rows, so they all were "approved"!
```

```{r}
#select by column: this will allow us to choose only columns we'll need for the rest of our analyses
names(nyt)
```
```{r}
nyt <- dplyr::select(nyt, c('commentID', 'commentBody', 'createDate', 
                            'recommendations', 'recipeName', 'recipeID', 
                            'replyCount', 'authorID', 'status'))

#remove an additional column
nyt <- dplyr::select(nyt, -c('status'))

#just for instructional purposes-- select columns with with certain titles using "matches" and "starts_with"
nyt_recipes <- dplyr::select(nyt, matches('recipe') | matches('comment'))
nyt_recipes <- dplyr::select(nyt, starts_with('recipe'))
nyt_IDs <- dplyr::select(nyt, ends_with('ID'))

#you can also use "select" to rearrange columns into a desired order:
nyt <- dplyr::select(nyt, c("recipeName", "recipeID"), everything())

head(nyt, n = 10)
```

## Using piping to make data wrangling more concise 

Piping in R is possible through the magittr package, which itself is included in the tidyverse packages. 

First save the raw data to a new dataframe called “nyt_concise”, and execute a more concise version of the steps taken above to reach the same result we have in our current “nyt” dataframe. 

```{r}
nyt_concise <- nyt_raw

nyt_concise <- nyt_concise %>% 
               filter(status == "approved") %>%
               dplyr::select(matches('recipe') | 
                            matches('commentID') | 
                            matches('commentBody') | 
                            matches('recommendations') | 
                            matches('createDate') | 
                            matches('replyCount')) %>%
               dplyr::rename(c('recipeName' = 'recipe_name',
                               'recipeID' = 'recipe_id'))

head(nyt_concise)

```

## Arrange 

Let’s say we wanted to have sort our data by date of the original comment. The comments come in unix date-time format, and we can use the as_datetime() function from the lubridate package to put the date into a readable format To arrange by date, we’ll use the arrange() function from dplyr, which arranges values in ascending order by default and can be reversed by adding the `desc() argument.

```{r}
#replace UNIX timestamps with interpretable date/time
nyt$createDate <- as_datetime(nyt$createDate)

#arrange by date (in ascending order by default: earliest dates first)
nyt <- arrange(nyt, createDate)

#arrange by date in descending order: latest dates first
nyt <- arrange(nyt, desc(createDate))

#select only reviews from 2021
nyt <- filter(nyt, createDate >= '2021-01-01 00:00:00') #greater than or equal to operator
nrow(nyt) #down from 389,963 to 53,355 reviews after cutting out all before 2021
```

```{r}
head(nyt, n = 10)

```

How would we add the arrange(), and filter() functions above into our “nyt_concise” dataframe?

```{r}
#changing the creation date to timedate format must be done separately
nyt_concise$createDate <-lubridate::as_datetime(nyt_concise$createDate)

nyt_concise <- nyt_concise %>%  
  arrange(createDate) %>%
  filter(createDate >= '2021-01-01 00:00:00')

head(nyt_concise)
```
#Mutating data, merging dataframes, exporting data

## Mutate and Transmute: create columns based on existing columns

To create new columns based on the content of other columns in a dataframe, we can use the mutate() function from the dplyr package. Using the transmute() function will achieve the same results as mutate() and is written the same way, but while mutate() will retain both the original and newly-created columns, transmute() keeps only the newly-created column. Here, we’ll create a new column containing the number of words in each comment called “textLength” and will keep the reference column (“commentBody”) by using mutate().


```{r}
### Create New Columns ###
#create a new column based on other columns
nyt <- mutate(nyt, textLength = qdap::word_count(nyt$commentBody)) #text length column

head(nyt, n = 10)
```
## Miscellaneous text cleaning
Since we have text data, I’m going to introduce a couple of functions for cleaning text data without getting too much into the details of Natural Language Processing (that workshop is in a few weeks, but feel free to check out this awesome online book called “Supervised Machine Learning for Text Analysis in R” by Emil Hvitfeldt & Julia Silge if you’re interested: https://smltar.com). I’ll first use the the str_trim() function from the stringr package to take out any extra characters encoded in the comment body, and will next create a dataframe called “nyt_text” containing only the comment body data. I will first tokenize the comments, next calculate sentiment scores using the “AFINN” dictionary, and lastly standardize these such that scores closer to +1 indicate stronger positive sentiment, while scores closer to -1 indicate stronger negative sentiment.

```{r}
#remove unwanted characters
nyt$commentBody <- str_trim(gsub("\n\n", "", nyt$commentBody)) 
nyt$commentBody <- str_trim(gsub("\n", "", nyt$commentBody)) 

#tokenize
nyt_text <- dplyr::select(nyt, c('commentBody', 'commentID'))
nyt_text <- nyt_text %>% unnest_tokens(word, commentBody) %>% #create tokens for each word
  anti_join(stop_words) #remove stop words using SMART lexicon
```

```{r}
nyt_text <- nyt_text %>% group_by(commentID) %>% summarise(commentBody = paste0(word, collapse = ' ')) #collapse back into original reviews

#get sentiment scores for each comment
nyt_text$sentiment <- get_sentiment(nyt_text$commentBody, method = "afinn")
head(nyt_text)
```

```{r}
#standardize sentiment scores
nyt_text$sentiment <- scale(nyt_text$sentiment, center=T, scale=T)

#rename tokenized comment body
nyt_text <- rename(nyt_text, c('commentToken' = 'commentBody'))

head(nyt_text)
```

# Join dataframes
Lastly, we will walk through dplyr’s functions for merging two dataframes together. We’ll use left_join() so that all of the columns from our newly-cleaned nyt_text dataframe can be matched with columns in our cleaned nyt dataframe. There are several different ways this could be achieved using dplyr’s join functions: left_join() (appends all rows in df y that match those in df x to df x), right_join() (appends all rows in df x that match df y to df y), inner_join() (appends all rows that match in df x and df y), and full_join() (appends all rows in df x and df y regardless of row match). These can each be helpful depending on the data structure you’re hoping to achieve.

```{r}
#left_join to merge nyt and nyt_text dataframes
nyt <- left_join(nyt, nyt_text, by=c('commentID'))

head(nyt, n = 20)
```
## Saving your cleaned data
Especially for large datasets like this one, it can be costly for your computer’s memory (and your time!) to rerun all of the cleaning steps above. By writing the cleaned data into a new csv file, you can load in your successfully wrangled data the next time you want to use it for analyses or visualizations.

```{r}
#if you don't want to rerun data cleaning steps, you can save and read in cleaned data directly
write.csv(nyt, "nyt_clean.csv") #this will save the cleaned file from above into a csv called "nyt_clean" in your working directory folder
```

# Application

## Exercise 2

For the second exercise, I’m going to ask you to use the functions above to answer a few specific questions about our final cleaned “nyt” dataset that might be similar to some exploratory steps you’d take on your own data.

## How many rows and columns are in the dataset?

```{r}
## How many rows and columns are in the dataset?
skim(nyt)
# 53355 rows
# 11 columns

```

```{r}
## What classes are the "commentBody" and "textLength" columns?
str(nyt)
# commentBody = character variable
# textLength columns = interval variable
```

```{r}
## Turn textLength into a numeric variable (suggestion: use as.numeric()).
nyt$textLength <- as.numeric(nyt$textLength)

str(nyt$textLength)
```
```{r}
## Use the skim() function to find the mean, SD, min and max for textLength, sentiment (Hint: we standardized sentiment earlier on, so M should be 0 and SD should be 1), and replyCount.

skim(nyt)

```

```{r}
## Remove rows containing outliers for textLength, sentiment, and replyCount (greater or less than 3SD from the mean)
nyt <- subset(nyt, textLength <= 116.23)
nyt <- subset(nyt, sentiment <= 3 & sentiment >= -3)
nyt <- subset(nyt, replyCount <= 1.58)

```


```{r}
## Remove any rows with missing data. (Hint: use drop_na())
nyt <- drop_na(nyt)
nrow(nyt) #52189 rows remaining

```

```{r}
## Plot histograms of the sentiment, textLength, and replyCount columns.
hist(nyt$sentiment)
hist(nyt$replyCount)
hist(nyt$textLength)
```

```{r}
## Using the select() function and datasummary_correlation() function from the modelsummary package, create an intercorrelation table of all numeric variables aside from date and ID variables.

names(nyt)
nyt_cor <- dplyr::select(nyt, c('recommendations', 'textLength', 'replyCount','sentiment'))
datasummary_correlation(nyt_cor)

```

## Small Group Exercise 3 & 4: Wrangling Data for Specific Analyses
The last exercise is going to be a bit of advanced application of what we’ve walked through so far, and which might require a bit more exploration of possible solutions online. Remember, there can be more than one way to achieve the same result, so it’s fine if your solution looks different than mine as long as the steps all make sense! If you’re interested, I encourage you to look into different ways to achieve some of these same results using the cheatsheets included in this workshop folder on Github, or search alternatives on Google or Stack Overflow. As a bonus, try to wrangle data as concisely as possible using the pipe functions!

Using a t-test via the t.test() function, compare comment sentiment for 2 different recipes. First, wrangle data into a usable format, and then perform the t-test. Feel free to use the help command in R (“?FUNCTION()) or the cheatsheets for any help. I’ve included suggested steps, but feel free to do whatever makes most sense to you.

```{r}
## Save nyt as new dataframe
nyt_analysis <- nyt
```

```{r}
## Call the recipeName function using the $ operator to choose recipe names
nyt_analysis$recipeName
```

```{r}
## Select needed columns
names(nyt_analysis)
nyt_analysis <- dplyr::select(nyt_analysis, c('recipeName', 'commentID', 'sentiment'))
                        
```

```{r}
## Use subset() or filter() to select rows for 2 recipe names
nyt_analysis <- subset(nyt_analysis, recipeName == 'grilled tofu' | recipeName == 'crab rangoon')

```


```{r}
## Use pivot_wider() (new function!) to transform data from long to wide format
nyt_analysis <- pivot_wider(nyt_analysis, id_cols = 'commentID', names_from = 'recipeName', values_from = 'sentiment')

```

```{r}
## Concise version of above steps:

### Concise ###
nyt_analysis <- nyt_analysis %>% dplyr::select(c('recipeName', 'sentiment', 'commentID')) %>% ## Select needed columns
  subset(recipeName == 'grilled tofu' | recipeName == 'crab rangoon') %>% ## Use subset() or filter() to select rows for 2 recipe names
  pivot_wider(id_cols = 'commentID', names_from = 'recipeName', values_from = 'sentiment') ## Use pivot_wider() (new function!) to transform data from long to wide format

```

```{r}
head(nyt_analysis, n = 20)

```

## Use t.test to compare sentiment for recipe 1 with recipe 2
```{r}
t.test(nyt_analysis$`grilled tofu`, nyt_analysis$`crab rangoon`,
       alternative = c('two.sided'),
       paired = F,
       conf.level = .95)
```
Sentiment does not differ between grilled tofu and crab rangoon

## Linear Regressions
Using two simple linear regressions via the lm() function, examine 
a) whether replyCount is predicted by sentiment, and 
b) whether textLength predicts sentiment. 

First, wrangle data into a usable format, and then perform the regressions. Feel free to use the help command in R (“?FUNCTION()) or the cheatsheets for any help. I’ve included suggested steps, but feel free to do whatever makes most sense to you
.

```{r}
## Save nyt as new dataframe
nyt_reg <- nyt
```

### Select needed columns 


```{r}
# view column names
names(nyt_reg)
```

```{r}

nyt_reg <- dplyr::select(nyt_reg, c('commentID', 'replyCount', 'textLength', 'sentiment'))
head(nyt_reg)
```


## Standardize replyCount and textLength

```{r}
nyt_reg$replyCount <- scale(nyt_reg$replyCount, center = T, scale = T)
nyt_reg$textLength <- scale(nyt_reg$textLength, center = T, scale = T)
```


## Use lm() and plot(effect()) to estimate linear regression models and visualize results
```{r}
## Use lm() and plot(effect()) to estimate linear regression models and visualize results

### replyCount ~ sentiment
lm1 <- lm(replyCount ~ sentiment, nyt_reg)
summary(lm1)
plot(effect("sentiment", lm1), grid = TRUE)
#Reply count is lower with stronger positive sentiment, and higher with stronger negative sentiment, b = -0.1, SE = 0.004, t = -22.43, p < .001.

### sentiment ~ textLength
lm2 <- lm(sentiment ~ textLength, nyt_reg)
summary(lm2)
plot(effect("textLength", lm2), grid = TRUE)
#More positive reviews were longer, b = 0.24, SE = 0.004, t = 65.18, p < .001.
```


